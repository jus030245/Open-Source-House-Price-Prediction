{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Work flow\nIt is always helpful to structure your work first. Details could be adjusted when you found some insights in data.\n1. EDA\n2. Feature Engineering\n3. Preprocessing for modeling\n4. Build model","metadata":{}},{"cell_type":"markdown","source":"> # 1. EDA","metadata":{}},{"cell_type":"markdown","source":"# 1.1 Get General Idea About the Data","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.head())\nprint(test.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('train shape',train.shape)\nprint('test shape',test.shape)\nprint('duplicated rows',train.duplicated().sum())\nprint('columns containing missing values',train.isnull().any().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['SalePrice'] = np.nan\ndata_all = pd.concat([train,test],ignore_index=True)\nprint('merged shape ',data_all.shape)\nprint(data_all.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Notes that it is important to distinguish the numerical variables from the categorical ones. \n* Here are some key takeaways:\n1. The dtype only give us a big picture about the data are stored. However, it is still important to see whether there are some 'int' data is actually ordinal or nominal. For example, in this case, 'Id' should be nominal (or not consider as a feature), 'MSSubClass' can be ordinal or nominal depending on how you analyse it.\n2. The corresponding visualization and analysis techniques can be different. For example, for numerical data, it is easy to compute the pearson correlation. On the other hand, using categories to draw different histogram is a nice way to show the distribution of our interested variable among the categories.\n3. Here we use dtype == ['int','float'] to find numerical variables and 'object' to find categories. Numerical variables can be further split into \"continuous\" ones such as height, and \"interval\" ones such as decibel. Categorical variable can be split into \"ordinal\" and \"nominal\". Sometimes, ordinal ones can be translate into numbers to show their ranking. Nominal ones may need to be processed by one-hot encoding, therefore leads to undesired high dimension. So dealing with High Cardinality nominal variables is also an important part in feature engineering.\n4. Time is an interesting feature, it would be helpful to store it as datatime64. However, first drawing out some lineplot against time would be nice to decide how to deal with it.","metadata":{}},{"cell_type":"code","source":"#This is a nice code to use at the very beginning\ndata_object = data_all.select_dtypes('object')\nprint('object shape ',data_object.shape)\ndata_num = data_all.select_dtypes(['int64','float64'])\nprint('num shape ',data_num.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_num.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in data_object.columns:\n    print(data_object[i].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Observed that some int features are not necessary continuous variable\n#Should have checked first ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"int_col = train.select_dtypes('int64').columns\ndef show_sample(column_list,row_range_start=0):\n    i = 0\n    while i < len(column_list):\n        try:\n            print(train[column_list[i:i+10]].iloc[row_range_start:row_range_start+10])\n            i += 10\n        except:\n            print(train[column_list[i:]].iloc[row_range_start:row_range_start+10])\nshow_sample(int_col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  1.1.1 Some little feature engineering","metadata":{}},{"cell_type":"markdown","source":"In my first trial, I didn't notice that this processing could be done first. But in this case, it would be more clear to show EDA with some feature processed.","metadata":{}},{"cell_type":"code","source":"discrete_int = ['Id','MSSubClass']\ntime_int = ['YrSold','YearBuilt','YearRemodAdd','MoSold']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[discrete_int] = train[discrete_int].astype('object')\ndata_all[discrete_int] = data_all[discrete_int].astype('object')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_num_features(df):\n    num = list(df.select_dtypes(['int64','float64']).columns)\n    try:\n        num.remove('SalePrice')\n    except:\n        pass\n    return num\ndef get_cat_features(df):\n     return list(df.select_dtypes('object').columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_train = train.select_dtypes(['int64','float64'])\nobject_train = train.select_dtypes('object')\nnum_train_corr = num_train.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_all = data_all.select_dtypes(['int64','float64'])\nobject_all = data_all.select_dtypes('object')\nnum_all_corr = num_all.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  1.2 EDA for Numerical Data","metadata":{}},{"cell_type":"code","source":"fig,ax=plt.subplots(figsize=(20,20))\nsns.heatmap(num_all_corr, cmap='Reds')\nplt.show()\nfig,ax=plt.subplots(figsize=(20,20))\nsns.heatmap(num_train_corr, cmap='Reds')\nplt.show()\n#1.Observed that OverallQual is correlated w/ lots of others\n#2.As well as TotalBsmtSF, 1stFlrSF, GrLivArea\n#3.Similar effects also happens to GarageYrBlt/Cars/Area\n#and it's obvious that they are highly correlated with each other\n#Lastly let's focus on the features that have the highest correlation with SalesPrice\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rank the sign and correlationship\nCorrelation = pd.DataFrame(num_train.corr()['SalePrice'])\nCorrelation['Abs'] = np.abs(Correlation['SalePrice'])\nCorrelation = Correlation.sort_values(by='Abs',ascending=False)\n#Most of the features are 'Positively' correlated with SalePrice\n#Now use 0.5 as a threshold to pick out the important ones\nimportant_features_CC = list(Correlation[Correlation['Abs'] > 0.5].index)\nimportant_features_CC.remove('SalePrice')\nprint(important_features_CC)\n#So now let's take a look at the details for the 10 mentioned ones in these sections\nfig,ax=plt.subplots(figsize=(20,20))\nsns.set(font_scale=1.5)\nsns.heatmap(train[important_features_CC+['SalePrice']].corr(),annot=True,annot_kws={\"size\": 20})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Since there are way too many variables with distinct definition\n#We can start by learning some background knowledge of the data through these features.\n#Here are some of the summary I would make at the beginning of EDA\ndata_all['GarageCars'].describe()\ndata_all['FullBath'].describe()\ndata_all['TotRmsAbvGrd'].describe()\ndata_all['YearBuilt'].describe()\ndata_all['YearRemodAdd'].describe()\n# \n# 'OverallQual', 1-10 ratings summarizing the house\n# 'GrLivArea', above ground area(feet^2)\n# 'GarageCars', cars capacity(0-5)\n# 'GarageArea', garage size area\n# 'TotalBsmtSF', basement area(feet^2)\n# '1stFlrSF', 1 flr area(feet^2)\n# 'FullBath', amount of bathrooms above ground(1-4)\n# 'TotRmsAbvGrd', amount of rooms above ground(2-15)\n# 'YearBuilt', construction date,1872-2010\n# 'YearRemodAdd', remodel date, 1950-2010\n# ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.3 EDA for Categorical Data","metadata":{}},{"cell_type":"code","source":"#histograms helps observed the distribution difference within groups\nobject_train['SalePrice'] = train['SalePrice']\nsns.set(font_scale=1)\nplt.rcParams[\"figure.figsize\"] = (10,6)\n\nfor i in object_train.columns:\n    if i in ['Id','SalePrice']:\n        pass\n    else:\n        categories = object_train[i].unique()\n        print('Categories for',i,\":\",len(categories))\n        sns.countplot(x=i,data=object_train)\n        plt.title(i)\n        plt.show()\n        for j in categories:\n            plt.hist(object_train[object_train[i] == j]['SalePrice'],alpha=0.5,label=j)\n        plt.legend(loc='upper right')\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(train['SalePrice'])\nplt.show()\n#SalePrice itself is right-skewed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> 1.4 Observe Time-related features","metadata":{}},{"cell_type":"code","source":"#Focus on Time related features\ndef draw_time(data,time_feature,y='SalePrice'):\n    frame_mean = data.groupby(time_feature)[y].mean()\n    frame_count =  data.groupby(time_feature)[y].count()\n    sns.lineplot(x=frame_mean.index,y=frame_mean)\n    plt.title('Mean '+y+' Against '+time_feature)\n    plt.show()\n    sns.lineplot(x=frame_count.index,y=frame_count)\n    plt.title('Count '+y+' Against '+time_feature)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_all['Sold_time'] = data_all['YrSold'].astype(str)+'/'+data_all['MoSold'].astype(str)\ndata_all['Sold_time'] = pd.to_datetime(data_all['Sold_time'] , format='%Y/%m')\ndata_all['MoSold'] = data_all['MoSold'].astype('object')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_time(data=data_all,time_feature='Sold_time')\ndraw_time(data=data_all,time_feature='YearBuilt')\ndraw_time(data=data_all,time_feature='YearRemodAdd')\n#seasonality found in sold time and price\n#trend found in remodel time and price","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # 2. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"> FEATURE ENGINEERING PROCESS\n1. For Categorical('Object')\n* Ordinal: change to int and consider scale\n* Nominal: Target the high cardinality one's and try to reduce dimension, one hot encoding, \n2. For numerical('int','float')\n* Change some 'int' into 'object' if it's actually ordinal or nominal (Shown in EDA)\n* Continous: see whether adjust the skewness, standardize\n* Interval: normalize\n* Time: seasonality, trend\n3. Deal with missing value\n4. Deal with ourliers\n5. Conduct the needed transformation mentioned above\n\n> Feature Selection\n* List out the ones that contribute nothing in EDA\n* First tryout the ones seem promising\n* Deal with multicolinearity or real-world relationship(Domain knowledge needed)","metadata":{}},{"cell_type":"markdown","source":"# 2.1 Feature Engineering for Categorical Features","metadata":{}},{"cell_type":"code","source":"#Change Ordinal features into numbers\ndata_all['ExterQual'] = data_all['ExterQual'].replace({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1})\ndata_all['ExterCond'] = data_all['ExterCond'].replace({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1})\ndata_all['BsmtQual'] = data_all['BsmtQual'].replace({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'NA':0})\ndata_all['BsmtCond'] = data_all['BsmtCond'].replace({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'NA':0})\ndata_all['BsmtExposure'] = data_all['BsmtExposure'].replace({'Gd':4,'Av':3,'Mn':2,'No':1,'NA':0})\ndata_all['HeatingQC'] = data_all['HeatingQC'].replace({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1})\ndata_all['KitchenQual'] = data_all['KitchenQual'].replace({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1})\ndata_all['FireplaceQu'] = data_all['FireplaceQu'].replace({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'NA':0})\ndata_all['GarageFinish'] = data_all['GarageFinish'].replace({'Fin':3,'RFn':2,'Unf':1,'NA':0})\ndata_all['GarageQual'] = data_all['GarageQual'].replace({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'NA':0})\ndata_all['GarageCond'] = data_all['GarageCond'].replace({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'NA':0})\ndata_all['PavedDrive'] = data_all['PavedDrive'].replace({'Y':3,'P':2,'N':1})\ndata_all['PoolQC'] = data_all['PoolQC'].replace({'Ex':4,'Gd':3,'TA':2,'Fa':1,'NA':0})\ndata_all['Fence'] = data_all['Fence'].replace({'GdPrv':4,'MnPrv':3,'GdWo':2,'MnWw':1,'NA':0})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pickout the categrorical features with high cardinality\nhigh_card_col_cat = [i for i in get_cat_features(data_all) if len(data_all[i].unique()) >= 10]\nhigh_card_col_cat.remove('Id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#build some new features based on our findings in EDA or domain\nfrom scipy import stats\nstats.pearsonr(data_all['ExterQual'],data_all['ExterCond'])\n#those two are independent variables\n#suggest that these two may have interaction effects\ndata_all['OverallValue'] = data_all['OverallQual'] * data_all['OverallCond']\ndata_all['ExterValue'] = data_all['ExterQual'] * data_all['ExterCond']\ndata_all['BsmtQual'] = data_all['BsmtQual'] * data_all['BsmtCond']\ndata_all['GarageValue'] = data_all['GarageQual'] * data_all['GarageCond']\ndata_all['TotalArea'] = data_all['TotalBsmtSF'] + data_all['1stFlrSF'] + data_all['2ndFlrSF']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data_all.groupby('MSSubClass')['SalePrice'].mean())\nprint(data_all.groupby('Exterior1st')['SalePrice'].mean())\nprint(data_all.groupby('Exterior2nd')['SalePrice'].mean())\nMSSubClass_Stories = {'20':1,'30':1,'40':1,'45':1.5,'50':1.5,'60':2,'70':2,'75':2.5,'120':1,'150':1.5,'160':2}\nMSSubClass_Ages = {'20':1,'30':0,'40':0.5,'45':0.5,'50':0.5,'60':1,'70':0,'75':0.5,'90':0.5,'120':1,'150':0.5,'160':1}\nMSSubClass_Other = []\ndef get_MSSubClass_Stories(data):\n    if str(data) in MSSubClass_Stories.keys():\n        return MSSubClass_Stories[str(data)]\n    else:\n        return 2\ndef get_MSSubClass_Ages(data):\n    if str(data) in MSSubClass_Ages.keys():\n        return MSSubClass_Ages[str(data)]\n    else:\n        return 0.5\n\n\ndata_all['Stories'] = data_all['MSSubClass'].apply(get_MSSubClass_Stories)\ndata_all['Ages'] = data_all['MSSubClass'].apply(get_MSSubClass_Ages)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Use data from outside, outside data can often help making decision if possible\n#this part is inspired by Mustafa Cicek, check out his amazing notebook as well:)\n\ngeo = {\"North\":[\"Blmngtn\", \"BrDale\", \"ClearCr\", \"Gilbert\",  \"Names\", \"NoRidge\", \"NPkVill\", \n           \"NoRidge\", \"NridgHt\", \"Sawyer\", \"Somerst\", \"StoneBr\", \"Veenker\", \"NridgHt\"],\n\n\"South\":[\"Blueste\", \"Edwards\", \"Mitchel\", \"MeadowV\", \"SWISU\", \"IDOTRR\", \"Timber\"],\n\"Downtown\":[\"BrkSide\", \"Crawfor\", \"OldTown\", \"CollgCr\"],\n\"West\":[\"Edwards\", \"NWAmes\", \"SWISU\", \"SawyerW\"]}\ndef find_geo(neighborhood):\n    for key, value in geo.items():\n        if neighborhood in value:\n            return key\n        else:\n            pass\n    return np.nan\ndata_all['Geo'] = data_all['Neighborhood'].apply(find_geo)\nprint(data_all.groupby('Geo')['SalePrice'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.2 Feature Engineering for Numerical Data","metadata":{}},{"cell_type":"code","source":"#fix skew for x\nfrom scipy.stats import skew\nskewed_feats = data_all[get_num_features(data_all)].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew_col = skewed_feats[abs(skewed_feats) > 0.5].index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in high_skew_col:\n    data_all[i] = np.log1p(data_all[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.3 Missing Values","metadata":{}},{"cell_type":"code","source":"###Missing Values\nmissing_counts = pd.DataFrame(data_all.isnull().sum().sort_values(ascending=False))\nplt.figure(figsize=(50,20))\nsns.heatmap(data_all.isnull())\nplt.show()\nplt.figure(figsize=(20,10))\nmissing_columns = missing_counts[missing_counts.iloc[:,0]>0]\nsns.barplot(x=missing_columns.index,y=missing_columns.iloc[:,0])\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#delete features with more than 1000 missing values\ndrop_col = list(missing_counts[missing_counts.iloc[:,0] > 1000].index)\ndrop_col.remove('SalePrice')\nmissing_columns = missing_columns.drop(index='SalePrice')\ntry:\n    data_all = data_all.drop(columns=drop_col,axis=0)\n    missing_columns = missing_columns.drop(index=drop_col,axis=1)\nexcept:\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data_all[missing_columns.index].info())\n#Since some numerical features are ordinal ones\n#it would be more reasonable to use median as the default values\n#as for categorical ones, we opt for the mode category\nmissing_object = data_all[missing_columns.index].select_dtypes('object').columns\nprint('missing object',len(missing_object))\nmissing_num = data_all[missing_columns.index].select_dtypes(['int64','float64']).columns\nprint('missing num ',len(missing_num))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in missing_num:\n    data_all[i] = data_all[i].fillna(data_all[i].median())\nfor j in missing_object:\n    data_all[j] = data_all[j].fillna(data_all[j].mode()[0])\nprint(data_all.isnull().any().sum())\n#1 missing is SalesPrice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.4 Extreme Values","metadata":{}},{"cell_type":"code","source":"###Extreme Values\n#here we only look at the important features discovered in EDA\nprint(data_all[important_features_CC].info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in train.columns:\n    if len(train[i].unique()) < 20:\n        sns.violinplot(x=train[i],y=train['SalePrice'])\n        plt.show()\n    else:\n        sns.scatterplot(x=train[i],y=train['SalePrice'])\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop only the two largest saleprice data\nextreme_ind = train[train['SalePrice'] > 700000].index\ndata_all = data_all.drop(index=extreme_ind,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get rid of non-related features\nlow_correl_col_num = list(Correlation[Correlation['Abs'] < 0.1].index)\ntry:\n    low_correl_col_num.remove('MoSold')\nexcept:\n    pass\nlow_correl_col_cat = ['Street','LotShape','Utilities','LotConfig','LandSlope','RoofStyle']\n#Possible Interaction\ndata_all = data_all.drop(columns=low_correl_col_num+low_correl_col_cat+['Id','Neighborhood','Sold_time','YrSold'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.5 Scaling and Transformation","metadata":{}},{"cell_type":"code","source":"all_num = get_num_features(data_all)\nall_cat = get_cat_features(data_all)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in all_num:\n    plt.hist(data_all[i])\n    plt.title(i)\n    plt.show()\n#ID not included\n#normalize all year feature\n#standardized all other numerical feature\n#log y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscale_col = ['YearRemodAdd','YearBuilt']\ndata_all[scale_col] = scaler.fit_transform(data_all[scale_col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Considering price should be postive, no trans needed\ndata_all['SalePrice'] = np.log(data_all['SalePrice'])\nsns.displot(data_all['SalePrice'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> #  3.Preprocessing for Model","metadata":{}},{"cell_type":"code","source":"#Processing and let ElasticNetRegression build benchmark\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\ndata_all_processed_x = pd.get_dummies(data_all)\ndata_all_processed_y = data_all['SalePrice']\ny_train = data_all['SalePrice'].dropna()\nX_train = data_all_processed_x[~data_all_processed_x['SalePrice'].isnull()].drop(columns='SalePrice')\nX_test = data_all_processed_x[data_all_processed_x['SalePrice'].isnull()].drop(columns='SalePrice')\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n                                    test_size=0.2, random_state=20210503)\nprint('X_train',X_train.shape)\nprint('X_val',X_val.shape)\nprint('X_test',X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # 4.Model Building","metadata":{}},{"cell_type":"markdown","source":"# 4.1 Try out ElasticNet","metadata":{}},{"cell_type":"markdown","source":"Here I use ElasticNet as a bencemary, since it is relatively easier to interpret and it will shrink the coef against collinearity. It would be nice startout to see what you've done above and have a gist of how well you've done. If most of the models won't work out, it may mean that further feature engineering is needed.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_squared_error\nElasticNet = ElasticNet(random_state=0,max_iter=5000)\n#scores = -cross_val_score(Ridge, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\nparameters = {'alpha':[0.001, 0.0001, 1e-05]}\nGrid = GridSearchCV(ElasticNet, parameters, cv=5, scoring='neg_root_mean_squared_error')\nresults = Grid.fit(X_train,y_train)\nprint('Best Para:',results.best_params_)\nprint('Best Score:',abs(results.best_score_))\n#print(scores,'\\n',np.mean(scores))\nprint('RMSE:',np.sqrt(mean_squared_error(y_val,results.best_estimator_.predict(X_val))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Send out Benchline without manually tuning and feature selection\nprediction = results.best_estimator_.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_Answer(prediction):\n    Answer = pd.DataFrame(np.exp(prediction))\n    Answer['Id'] = Answer.index + 1461\n    Answer.columns = ['SalePrice','Id']\n    Answer = Answer[['Id','SalePrice']]\n    return Answer\nAnswer = get_Answer(prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Answer.to_csv('Submit.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Take a look at the contribution of the features, does it look similar to what you've expected in EDA.:)?\n","metadata":{}},{"cell_type":"code","source":"def get_contribution(estimator):\n    Contribution = pd.DataFrame(np.abs(estimator.best_estimator_.coef_))\n    Contribution = Contribution.sort_values(by=0,ascending=False)\n    Contribution.index = X_train.columns\n    Contribution.columns = ['coef']\n    return Contribution\ndef visualize_contribution(df):\n    fig,ax=plt.subplots(figsize=(40,20))\n    plt.bar(x=df.index,height=df['coef'])\n    plt.title('Feature Contribution')\n    plt.xticks(rotation=90,fontsize=22)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Contribution = get_contribution(results)\nvisualize_contribution(Contribution[Contribution['coef']>0])\nvisualize_contribution(Contribution[Contribution['coef']>0.05])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* My benchmark is Ranking 38%, RMSE=0.13 (This is done with just basic data cleansing and no hyperparameter tuning)\n* Observed that the RMSE is pretty close(better) to the fitting result\n\n# Steps can be done when the score is not what you desire\n* Feature Selection (Create new features)\n* Different Preprocessing measure\n* Try different Models\n* Model Tuning\n* Emsemble methods / Stacking","metadata":{}},{"cell_type":"markdown","source":"# 4.2 Different Models and scoring metrics","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import SCORERS\nSCORERS.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#1.Tryout Different Models\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge, Lasso\nscoring = {'R^2_ad':'r2',\n          'R^2':'explained_variance',\n          'RMSE':'neg_root_mean_squared_error'}\nDecisionTreeRegressor = DecisionTreeRegressor(random_state=0)\nprint('DecisionTree \\n',cross_validate(DecisionTreeRegressor, X_train,y_train, scoring=scoring, cv=5, return_train_score=True))\nLGBMRegressor = LGBMRegressor(random_state=0)\nprint('LGBMRegressor \\n',cross_validate(LGBMRegressor, X_train,y_train, scoring=scoring, cv=5, return_train_score=True))\nXGBRegressor = XGBRegressor()\nprint('XGBRegressor \\n',cross_validate(XGBRegressor, X_train,y_train, scoring=scoring, cv=5, return_train_score=True))\nSVR = SVR()\nprint('SVR \\n',cross_validate(SVR, X_train,y_train, scoring=scoring, cv=5, return_train_score=True))\nRandomForestRegressor = RandomForestRegressor()\nprint('SVR \\n',cross_validate(RandomForestRegressor, X_train,y_train, scoring=scoring, cv=5, return_train_score=True))\nRidge = Ridge(random_state=0)\nprint('Ridge \\n',cross_validate(Ridge, X_train,y_train, scoring=scoring, cv=5, return_train_score=True))\nLasso = Lasso(random_state=0)\nprint('Lasso \\n',cross_validate(Lasso, X_train,y_train, scoring=scoring, cv=5, return_train_score=True))\n#lgbm seems to have a better performance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = results.best_estimator_.predict(X_test)\nAnswer =  get_Answer(prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Answer.to_csv('Submit.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nlgb_model = lgb.LGBMRegressor(colsample_bytree=0.25, learning_rate=0.01,\n                              max_depth=13, min_child_samples=7, n_estimators=10000,\n                              num_leaves=20, objective='regression', random_state=42)\nxgb_model = xgb.XGBRegressor(colsample_bytree=0.25, gamma=0.0, learning_rate=0.01, max_depth=3,\n                             n_estimators=15000, n_jobs=-1, random_state=42, \n                             reg_alpha=0.25, reg_lambda=0.4, subsample=1.0)\ngbr_model = GradientBoostingRegressor(alpha=0.9,\n                                      learning_rate=0.01, loss='huber',\n                                      max_depth=13, max_features=0.1, min_samples_split=110,\n                                      n_estimators=10000, n_iter_no_change=100, random_state=42)\nsvr_model = SVR(C=0.75, coef0=0.0001, degree=2, epsilon=0.0001, gamma=0.005, max_iter=10000)\nlasso_model = Lasso(alpha=0.0001, max_iter=5000, random_state=42)\nridge_model = Ridge(alpha=2.5, max_iter=5000, random_state=42)\nenet_model = ElasticNet(alpha=0.0002, l1_ratio=0.65, max_iter=5000, random_state=42)\nmodels = [lgb_model,xgb_model,gbr_model,svr_model,lasso_model,ridge_model,enet_model]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in models:\n    i.fit(X_train,y_train)\n    print(str(i)+'RMSE:',np.sqrt(mean_squared_error(y_val,i.predict(X_val))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.3 Ensemble Methods and Submission","metadata":{}},{"cell_type":"code","source":"#Simple Ensemble with the top 3 algorithm\nxgb_model.fit(pd.concat([X_train,X_val]),pd.concat([y_train,y_val]))\nlgb_model.fit(pd.concat([X_train,X_val]),pd.concat([y_train,y_val]))\ngbr_model.fit(pd.concat([X_train,X_val]),pd.concat([y_train,y_val]))\nprediction = xgb_model.predict(X_test) * 0.4 + lgb_model.predict(X_test) * 0.3 + gbr_model.predict(X_test) * 0.3 \nget_Answer(prediction).to_csv('0511_0237.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This result ends up scoring Top15% with 0.12301 RMSE","metadata":{}},{"cell_type":"markdown","source":"Here is a sample code for Stacking. Since it's a practice I opt for simple average-ish emsemble method.","metadata":{}},{"cell_type":"code","source":"#Stacking\nfrom sklearn.ensemble import StackingRegressor\nbase_models = [('Elastic',ElasticNet()),\n             ('SVR',SVR()),\n             ('XGB',XGBRegressor),\n             ('DecisionTree',DecisionTreeRegressor),\n              ('RandomForest',RandomForestRegressor)]\n\nStacking = StackingRegressor(\n     estimators=base_models,\n     final_estimator=LGBMRegressor)\n\nStacking.fit(X_train,y_train)\nprint('RMSE:',np.sqrt(mean_squared_error(y_val,Stacking.predict(X_val))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}